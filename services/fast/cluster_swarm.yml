version: "3"

##########################################################################################

x-single-network: &single_network
  networks:
    - layer_fast_prod
    
x-multi-network: &multi_network
  networks:
    - layer_fast_prod
    - layer_batch_prod

x-kafka-common-vars: &kafka_commons_vars
  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
  KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081

##########################################################################################
#########################    DEPLOYMENT CONFIGS FOR NODES    #############################

x-common-restart-default: &common_restart_policy
  restart_policy:
    condition: on-failure


x-common-deploy-master: &common_deploy_master
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-desktop]

x-common-deploy-worker-1: &common_deploy_worker_1
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-HP-ZBook-15-G2]

x-common-deploy-worker-2: &common_deploy_worker_2
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-server-2]

x-common-deploy-worker-3: &common_deploy_worker_3
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-server]
##########################################################################################
################    DEFINIÇÃO DOS SERVIÇOS    ###################
#################################################################

services:

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    <<: *single_network
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 3000
    volumes:
    - type: volume
      source: dm_cluster_prod_zookeeper_data
      target: /var/lib/zookeeper/data
    - type: volume
      source: dm_cluster_prod_zookeeper_log
      target: /var/lib/zookeeper/log
    - type: volume
      source: dm_cluster_prod_zookeeper_secrets
      target: /etc/zookeeper/secrets
    <<: *common_deploy_worker_1

  ################################################################################
  ######################    BEGIN KAFKA SERVICES    ##############################

  broker-1:
    image: confluentinc/cp-server:7.6.0
    <<: *single_network
    environment:
      KAFKA_BROKER_ID: 1
      <<: *kafka_commons_vars
      KAFKA_LISTENERS: INTERNAL://:29092,OUTSIDE://:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29092,OUTSIDE://host.docker.internal:9092
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker-1:29092  
    volumes:
    - type: volume
      source: dm_cluster_prod_kafka_1_data
      target: /var/lib/kafka/data
    - type: volume
      source: dm_cluster_prod_kafka_1_secrets
      target: /etc/kafka/secrets
    <<: *common_deploy_worker_1

  broker-2:
    image: confluentinc/cp-server:7.6.0
    <<: *single_network
    environment:
      KAFKA_BROKER_ID: 2
      <<: *kafka_commons_vars
      KAFKA_LISTENERS: INTERNAL://:29093,OUTSIDE://:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29093,OUTSIDE://host.docker.internal:9093
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker-2:29093
    volumes:
    - type: volume
      source: dm_cluster_prod_kafka_2_data
      target: /var/lib/kafka/data
    - type: volume
      source: dm_cluster_prod_kafka_2_secrets
      target: /etc/kafka/secrets
    <<: *common_deploy_worker_2

  broker-3:
    image: confluentinc/cp-server:7.6.0
    <<: *single_network
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      <<: *kafka_commons_vars
      KAFKA_LISTENERS: INTERNAL://:29094,OUTSIDE://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29094,OUTSIDE://host.docker.internal:9094
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker-3:29094  
    volumes:
    - type: volume
      source: dm_cluster_prod_kafka_3_data
      target: /var/lib/kafka/data
    - type: volume
      source: dm_cluster_prod_kafka_3_secrets
      target: /etc/kafka/secrets
    <<: *common_deploy_worker_3

  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.0
    hostname: schema-registry
    <<: *single_network
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "broker-1:29092,broker-2:29093,broker-3:29094"
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    <<: *common_deploy_worker_2

  kafka-connect:
    image: confluentinc/cp-kafka-connect-base:7.6.0
    hostname: kafka-connect
    <<: *multi_network
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker-1:29092,broker-2:29093,broker-3:29094'
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
    <<: *common_deploy_worker_2

  # KAFKA UI -> http://192.168.15.101:9021
  control-center:
    image: confluentinc/cp-enterprise-control-center:7.6.0
    hostname: control-center
    <<: *single_network
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker-1:29092,broker-2:29093,broker-3:29094'
      CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      CONTROL_CENTER_CONNECT_CLUSTER: 'kafka-connect:8083'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      PORT: 9021
    <<: *common_deploy_master

  ################################################################################
  ######################    BEGIN SPARK SERVICES    ##############################

  # SPARK UI -> http://192.168.15.101:18080
  spark-master:
    image: docker.io/bitnami/spark:3.5
    <<: *single_network
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - '18080:8080'
    <<: *common_deploy_master

  spark-worker-1:
    image: docker.io/bitnami/spark:3.5
    <<: *single_network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    <<: *common_deploy_worker_2

  spark-worker-2:
    image: docker.io/bitnami/spark:3.5
    <<: *single_network
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    <<: *common_deploy_worker_3

  ################################################################################
  ######################    BEGIN DATABASE SERVICES    ###########################

  scylladb:
    image: marcoaureliomenezes/dm-scylladb:1.0.0
    <<: *single_network
    restart: always
    ports:
      - "9042:9042"
    volumes:
      - 'dm_cluster_prod_scylladb_data:/var/lib/scylla'
    <<: *common_deploy_master

  redis:
    image: redis:latest
    <<: *single_network
    <<: *common_deploy_worker_2

  # REDIS UI -> http://192.168.15.101:18081
  redis-commander:
    image: rediscommander/redis-commander:latest
    <<: *single_network
    environment:
      - REDIS_HOSTS=local:redis:6379
    ports:
      - "18081:8081"
    depends_on:
      - redis
    <<: *common_deploy_worker_3

  # VISUALIZER SWARM -> http://192.168.15.101:28080/
  visualizer:
    image: dockersamples/visualizer:stable
    <<: *single_network
    ports:
      - "28080:8080"
    stop_grace_period: 1m30s
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    <<: *common_deploy_master


  # # APACHE HADOOP SERVICES
  # namenode:
  #   image: marcoaureliomenezes/dm-hadoop-namenode:1.0.0
  #   networks:
  #     - layer_fast_prod
  #   ports:
  #     - 9870:9870
  #     - 9000:9000
  #   volumes:
  #     - hadoop_namenode_prod:/hadoop/dfs/name
  #   env_file:
  #     - ./hadoop.env
  #   environment:
  #     - CLUSTER_NAME=datalake
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.hostname == dadaia-server-2]

  # datanode:
  #   image: marcoaureliomenezes/dm-hadoop-datanode:1.0.0
  #   networks:
  #     - layer_fast_prod
  #   ports:
  #     - 9864:9864
  #   volumes:
  #     - hadoop_datanode_prod_1:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #   env_file:
  #     - ./hadoop.env
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.hostname == dadaia-server-2]




volumes:
  dm_cluster_prod_zookeeper_data:
  dm_cluster_prod_zookeeper_log:
  dm_cluster_prod_zookeeper_secrets:
  dm_cluster_prod_kafka_1_data:
  dm_cluster_prod_kafka_1_secrets:
  dm_cluster_prod_kafka_2_data:
  dm_cluster_prod_kafka_2_secrets:
  dm_cluster_prod_kafka_3_data:
  dm_cluster_prod_kafka_3_secrets:
  dm_cluster_prod_scylladb_data:
  #hadoop_namenode_prod:
  #hadoop_historyserver_prod:
  #hadoop_datanode_prod_1:

networks:
  layer_fast_prod:
    external: true
  layer_batch_prod:
    external: true

version: '3'


##########################################################################################

x-common-log-config: &default_config
  networks:
    - layer_batch_prod
  logging:
    driver: "json-file"
    options:
      max-file: "5"
      max-size: "10m"

##########################################################################################
#########################    DEPLOYMENT CONFIGS FOR NODES    #############################

x-common-restart-default: &common_restart_policy
  restart_policy:
    condition: on-failure


x-common-deploy-master: &common_deploy_master
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-desktop]

x-common-deploy-worker-1: &common_deploy_worker_1
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-HP-ZBook-15-G2]

x-common-deploy-worker-2: &common_deploy_worker_2
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-server-2]

x-common-deploy-worker-3: &common_deploy_worker_3
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-server]

# ==============================================================
# =================  DEFINIÇÃO DOS SERVIÇOS  ===================
# ==============================================================
services:

  # APACHE HADOOP SERVICES
  namenode:
    image: marcoaureliomenezes/dm-hadoop-namenode:1.0.0
    <<: *default_config
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode_prod_vol:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=datalake
    <<: *common_deploy_worker_1

  resourcemanager:
    image: marcoaureliomenezes/dm-hadoop-resourcemanager:1.0.0
    <<: *default_config
    ports:
      - 18088:8088
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_1

  datanode-1:
    image: marcoaureliomenezes/dm-hadoop-datanode:1.0.0
    <<: *default_config
    volumes:
      - hadoop_datanode_prod_vol_1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_1

  datanode-2:
    image: marcoaureliomenezes/dm-hadoop-datanode:1.0.0
    <<: *default_config
    volumes:
      - hadoop_datanode_prod_vol_2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_2


  datanode-3:
    image: marcoaureliomenezes/dm-hadoop-datanode:1.0.0
    <<: *default_config
    volumes:
      - hadoop_datanode_prod_vol_3:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_3

  nodemanager-1:
    image: marcoaureliomenezes/dm-hadoop-nodemanager:1.0.0
    <<: *default_config
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088 datanode-1:9864 datanode-2:9864 datanode-3:9864"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_1

  nodemanager-2:
    image: marcoaureliomenezes/dm-hadoop-nodemanager:1.0.0
    <<: *default_config
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088 datanode-1:9864  datanode-2:9864 datanode-3:9864"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_2

  nodemanager-3:
    image: marcoaureliomenezes/dm-hadoop-nodemanager:1.0.0
    <<: *default_config
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088 datanode-1:9864 datanode-2:9864 datanode-3:9864"
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_3

  historyserver:
    image: marcoaureliomenezes/dm-hadoop-historyserver:1.0.0
    <<: *default_config
    ports:
      - 18188:8188
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088 datanode-1:9864 datanode-2:9864 datanode-3:9864"
    volumes:
      - hadoop_histserver_prod_vol:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    <<: *common_deploy_worker_1

  # APACHE HIVE SERVICES AND HUE
  
  # POSTGRES DATABASE FOR HIVE METASTORE AND HUE
  postgres:
    image: marcoaureliomenezes/dm-postgres:1.0.0
    <<: *default_config
    environment:
      - POSTGRES_USER=postgres
    volumes:
      - pg_hive_hue_prod:/var/lib/postgresql/data
    <<: *common_deploy_worker_3


  hive-metastore:
    image: marcoaureliomenezes/dm-hive-base:1.0.0
    <<: *default_config
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode-1:9864 datanode-2:9864 datanode-3:9864 postgres:5432"
    ports:
      - "9083:9083"
    env_file:
      - ./hadoop-hive.env
    <<: *common_deploy_worker_1

  hive-server:
    image: marcoaureliomenezes/dm-hive-base:1.0.0
    <<: *default_config
    env_file:
      - ./hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://postgres:5432/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    <<: *common_deploy_worker_1


  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    <<: *default_config
    deploy:
      <<: *common_restart_policy
      placement:
        constraints:
          - node.hostname == dadaia-HP-ZBook-15-G2

  hue-webui:
    image: marcoaureliomenezes/dm-hue-webui:1.0.0
    <<: *default_config
    ports:
      - "32762:8888"
    # volumes:
    #   - ../mnt/hue/hue.ini:/usr/share/hue/desktop/conf/z-hue.ini
    environment:
      SERVICE_PRECONDITION: "hive-server:10000 postgres:5432"
    <<: *common_deploy_master


  # APACHE SPARK SERVICES
  # spark-master:
  #   image: marcoaureliomenezes/dm-spark-master:1.0.0
  #   <<: *default_config
  #   ports:
  #     - "8080:8082"
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.hostname == dadaia-server-2]

  # spark-worker:
  #   image: marcoaureliomenezes/dm-spark-worker:1.0.0
  #   <<: *default_config
  #   depends_on:
  #     - spark-master
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #     placement:
  #       constraints: [node.hostname == dadaia-server-2]
      
      
  airflow:
    image: marcoaureliomenezes/dm-apache-airflow:1.0.0
    <<: *default_config
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ../../mnt/airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ../../mnt/airflow/dags:/opt/airflow/dags
    ports:
      - 8080:8080
    <<: *common_deploy_master

  postgres_airflow:
    image: postgres:11.4-alpine
    <<: *default_config
    volumes:
      - pg_airflow_prod:/var/lib/postgresql/data/pgdata
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow_db
      - PGDATA=/var/lib/postgresql/data/pgdata
    <<: *common_deploy_worker_1


networks:
  layer_batch_prod:
    external: true

volumes:
  hadoop_namenode_prod_vol:
  hadoop_datanode_prod_vol_1:
  hadoop_datanode_prod_vol_2:
  hadoop_datanode_prod_vol_3:
  hadoop_histserver_prod_vol:
  pg_hive_hue_prod:
  pg_airflow_prod:
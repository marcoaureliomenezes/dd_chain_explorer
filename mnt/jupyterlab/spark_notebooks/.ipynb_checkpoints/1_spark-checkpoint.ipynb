{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e2b48915-951a-422a-a0b8-06e7ec4155a2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 947ms :: artifacts dl 31ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e2b48915-951a-422a-a0b8-06e7ec4155a2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/22ms)\n",
      "24/10/01 02:46:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a73823c7f6b7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>iceberg_hello_world</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b92f4b98bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.spark import get_spark_session\n",
    "import os\n",
    "\n",
    "spark = get_spark_session(\"iceberg_DDL\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_fast`.`all_topics` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 19;\n'DescribeRelation true, [col_name#0, data_type#1, comment#2]\n+- 'UnresolvedTableOrView [bronze_fast, all_topics], DESCRIBE TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m sc_client \u001b[38;5;241m=\u001b[39m SchemaRegistryUtils\u001b[38;5;241m.\u001b[39mget_schema_registry_client(SCHEMA_REGISTRY_URL)\n\u001b[1;32m     82\u001b[0m avro_schema_logs \u001b[38;5;241m=\u001b[39m SchemaRegistryUtils\u001b[38;5;241m.\u001b[39mget_avro_schema(sc_client, SCHEMA_REGISTRY_SUBJECT)\n\u001b[0;32m---> 84\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDESCRIBE FORMATTED bronze_fast.all_topics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     85\u001b[0m data_extracted \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mextract_data(avro_schema_logs)\n\u001b[1;32m     86\u001b[0m data_transformed \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mtransform_data(data_extracted)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_fast`.`all_topics` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 19;\n'DescribeRelation true, [col_name#0, data_type#1, comment#2]\n+- 'UnresolvedTableOrView [bronze_fast, all_topics], DESCRIBE TABLE, true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql.functions import col, expr, split, window, count, max\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "from utils.schema_registry_utils import SchemaRegistryUtils\n",
    "\n",
    "class APIKeyMonitor:\n",
    "   \n",
    "  def __init__(self, spark, kafka_options):\n",
    "    self.spark = spark\n",
    "    self.kafka_options = kafka_options\n",
    "\n",
    "\n",
    "  def extract_data(self, avro_schema: str):\n",
    "    df_simple_transactions = (\n",
    "      self.spark\n",
    "        .readStream\n",
    "        .format(\"kafka\")\n",
    "        .options(**self.kafka_options)\n",
    "        .load()\n",
    "        .withColumn(\"offset\", col(\"offset\").cast(IntegerType()))\n",
    "        .withColumn(\"timestamp\", col(\"timestamp\").cast(StringType()))\n",
    "        .withColumnRenamed(\"key\", \"topic_key\")\n",
    "        .withColumnRenamed(\"value\", \"topic_value\")\n",
    "        .withColumnRenamed(\"timestamp\", \"kafka_timestamp\")\n",
    "        .withColumnRenamed(\"partition\", \"kafka_partition\")\n",
    "        .select(\"topic_key\", \"topic_value\",  \"kafka_partition\", \"offset\",\n",
    "                \"kafka_timestamp\", \"timestamptype\", \"topic\"))\n",
    "    return df_simple_transactions\n",
    "\n",
    "\n",
    "  def transform_data(self, df_stream):\n",
    "    return df_stream\n",
    "\n",
    "\n",
    "  def load_data_to_console(self, df_transformed):\n",
    "    write_stream_query = (\n",
    "      df_transformed\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"console\")\n",
    "        .start()\n",
    "        .awaitTermination()\n",
    "    )\n",
    "    return write_stream_query\n",
    "\n",
    "  def load_data_to_bronze(self, df_transformed):\n",
    "    query = (\n",
    "      df_transformed\n",
    "        .writeStream\n",
    "        .format(\"iceberg\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"hdfs://namenode:9000/dm_lakehouse/checkpoint/all_topics\")\n",
    "        .toTable(\"bronze_fast.all_topics\")\n",
    "    )\n",
    "    return query\n",
    "\n",
    "\n",
    "APP_NAME = \"Handle_Simple_Transactions\"\n",
    "SPARK_URL = os.getenv(\"SPARK_MASTER_URL\")\n",
    "\n",
    "KAFKA_CLUSTER = os.getenv(\"KAFKA_BROKERS\", \"broker:29092\")\n",
    "CONSUMER_GROUP = os.getenv(\"CG_API_KEY_CONSUME\", \"cg_war\")\n",
    "STARTING_OFFSETS = os.getenv(\"STARTING_OFFSETS\", \"latest\")\n",
    "MAX_OFFSETS_PER_TRIGGER = os.getenv(\"MAX_OFFSETS_PER_TRIGGER\", 1000)\n",
    "\n",
    "SCHEMA_REGISTRY_URL = \"http://schema-registry:8081\"\n",
    "SCHEMA_REGISTRY_SUBJECT = \"mainnet.application.logs-value\"\n",
    "\n",
    "kafka_options = {\n",
    "\"kafka.bootstrap.servers\": KAFKA_CLUSTER,\n",
    "\"subscribe\": \"mainnet.mined.block.metadata,mainnet.mined.txs.token.transfer\",\n",
    "\"startingOffsets\": STARTING_OFFSETS,\n",
    "\"group.id\": CONSUMER_GROUP,\n",
    "\"maxOffsetsPerTrigger\": MAX_OFFSETS_PER_TRIGGER \n",
    "}\n",
    "\n",
    "engine = APIKeyMonitor(spark, kafka_options)\n",
    "sc_client = SchemaRegistryUtils.get_schema_registry_client(SCHEMA_REGISTRY_URL)\n",
    "avro_schema_logs = SchemaRegistryUtils.get_avro_schema(sc_client, SCHEMA_REGISTRY_SUBJECT)\n",
    "\n",
    "spark.sql(\"DESCRIBE FORMATTED bronze_fast.all_topics\").show()\n",
    "data_extracted = engine.extract_data(avro_schema_logs)\n",
    "data_transformed = engine.transform_data(data_extracted)\n",
    "#query = engine.load_data_to_bronze(data_transformed)\n",
    "query = engine.load_data_to_console(data_transformed)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

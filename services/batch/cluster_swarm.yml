version: '3'


x-common-log-config: &default_config
  networks:
    - dm_cluster_prod-fast
  logging:
    driver: "json-file"
    options:
      max-file: "5"
      max-size: "10m"

x-common-restart-default: &common_restart_policy
  restart_policy:
    condition: on-failure

x-common-deploy-master: &common_deploy_master
  deploy:
    <<: *common_restart_policy
    placement:
      constraints:
        - node.hostname == dadaia-desktop

x-common-deploy-worker-1: &common_deploy_worker_1
  deploy:
    <<: *common_restart_policy
    placement:
      constraints:
        - node.hostname == dadaia-HP-ZBook-15-G2

x-common-deploy-worker-2: &common_deploy_worker_2
  deploy:
    <<: *common_restart_policy
    placement:
      constraints:
        - node.hostname == dadaia-server

x-common-deploy-worker-3: &common_deploy_worker_3
  deploy:
    <<: *common_restart_policy
    placement:
      constraints: [node.hostname == dadaia-server-2]

# ==============================================================
# =================  DEFINIÇÃO DOS SERVIÇOS  ===================
# ==============================================================
services:

  # APACHE HADOOP SERVICES
  namenode:
    image: marcoaureliomenezes/dm-hadoop-namenode:1.0.0
    networks:
      - dm_cluster_prod-fast
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode_prod:/hadoop/dfs/name
    env_file:
      - ./hadoop.env
    environment:
      - CLUSTER_NAME=datalake
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]

  datanode:
    image: marcoaureliomenezes/dm-hadoop-datanode:1.0.0
    networks:
      - dm_cluster_prod-fast
    ports:
      - 9864:9864
    volumes:
      - hadoop_datanode_prod_1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]


  resourcemanager:
    image: marcoaureliomenezes/dm-hadoop-resourcemanager:1.0.0
    networks:
      - dm_cluster_prod-fast
    ports:
      - 18088:8088
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]


  nodemanager:
    image: marcoaureliomenezes/dm-hadoop-nodemanager:1.0.0
    networks:
      - dm_cluster_prod-fast
    ports:
      - 18042:8042
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]


  historyserver:
    image: marcoaureliomenezes/dm-hadoop-historyserver:1.0.0
    networks:
      - dm_cluster_prod-fast
    ports:
      - 19888:8188
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver_prod:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]

  # APACHE HIVE SERVICES AND HUE
  # hive-metastore:
  #   image: marcoaureliomenezes/dm-hive-base:1.0.0
  #   <<: *default_config
  #   command: /opt/hive/bin/hive --service metastore
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870 datanode:9864 postgres:5432"
  #   ports:
  #     - "9083:9083"
  #   env_file:
  #     - ./hadoop-hive.env
  #   <<: *common_deploy_master


  # hive-server:
  #   image: marcoaureliomenezes/dm-hive-server:1.0.0
  #   <<: *default_config
  #   environment:
  #     HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://postgres:5432/metastore"
  #     SERVICE_PRECONDITION: "hive-metastore:9083"
  #   env_file:
  #     - ./hadoop-hive.env
  #   <<: *common_deploy_master


  # presto-coordinator:
  #   image: shawnzhu/prestodb:0.181
  #   <<: *default_config
  #   deploy:
  #     <<: *common_restart_policy
  #     placement:
  #       constraints:
  #         - node.hostname == dadaia-HP-ZBook-15-G2

  # postgres:
  #   image: marcoaureliomenezes/dm-postgres:1.0.0
  #   <<: *default_config
  #   volumes:
  #     - postgres_hive_hue_prod:/var/lib/postgresql/data
  #   <<: *common_deploy_master


  # hue-webui:
  #   image: marcoaureliomenezes/dm-hue-webui:1.0.0
  #   <<: *default_config
  #   ports:
  #     - "32762:8888"
  #   volumes:
  #     - ../mnt/hue/hue.ini:/usr/share/hue/desktop/conf/z-hue.ini
  #   environment:
  #     SERVICE_PRECONDITION: "hive-server:10000 postgres:5432"
  #   <<: *common_deploy_master


  # APACHE SPARK SERVICES
  spark-master:
    image: marcoaureliomenezes/dm-spark-master:1.0.0
    networks:
      - dm_cluster_prod-fast
    ports:
      - "8080:8082"
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]

  spark-worker:
    image: marcoaureliomenezes/dm-spark-worker:1.0.0
    networks:
      - dm_cluster_prod-fast
    depends_on:
      - spark-master
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.hostname == dadaia-server-2]
      
      
  # airflow:
  #   image: marcoaureliomenezes/dm-apache-airflow:1.0.0
  #   <<: *default_config
  #   volumes:
  #     - ../mnt/airflow/airflow.cfg:/opt/airflow/airflow.cfg
  #     - ../mnt/airflow/dags:/opt/airflow/dags
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   ports:
  #     - 8080:8080
  #   <<: *common_deploy_master

  # postgres_airflow:
  #   image: postgres:11.4-alpine
  #   container_name: postgres_airflow
  #   <<: *default_config
  #   volumes:
  #     - postgres_airflow_prod:/var/lib/postgresql/data/pgdata
  #   environment:
  #     - POSTGRES_USER=airflow
  #     - POSTGRES_PASSWORD=airflow
  #     - POSTGRES_DB=airflow_db
  #     - PGDATA=/var/lib/postgresql/data/pgdata
  #   <<: *common_deploy_worker_2


networks:
  dm_cluster_prod-fast:
    driver: overlay

volumes:
  hadoop_namenode_prod:
  hadoop_historyserver_prod:
  hadoop_datanode_prod_1:
  # postgres_hive_hue_prod:
  # postgres_airflow_prod: